sparse temporal difference learning via alternating direction method multiplier approximation algorithm context optimization function approximation learning artificial intelligence prediction algorithm linear programming recent work off line reinforcement learning focused efficient algorithm incorporate feature selection via l1 regularization into bellman operator fixed point estimator these development now mean that over fitting avoided when number sample small compared number feature however it remains unclear whether existing algorithm ability offer good approximation task policy evaluation improvement this paper propose new algorithm approximating fixed point based alternating direction method multiplier admm demonstrate with experimental result that proposed algorithm more stable policy iteration compared prior work furthermore also derive theoretical result that state proposed algorithm obtains solution which satisfies optimality condition fixed point problem 