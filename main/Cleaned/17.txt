state abstraction reinforcement learning by eliminating useless dimension reinforcement learning state abstraction intelligent agent complexity reduction q learning other linear dynamic learning algorithm subject bellman curse dimensionality any realistic learning problem this paper introduces framework satisficing state abstraction one that reduces state dimensionality improving convergence reducing computational memory resource by eliminating useless state dimension statistical parameter that dependent state q value identify relevance given state space task space allow state element that contribute least task learning discarded empirical result applying state abstraction canonical single agent path planning task more difficult multi agent foraging problem demonstrate utility proposed method improving learning convergence performance resource constrained learning problem 