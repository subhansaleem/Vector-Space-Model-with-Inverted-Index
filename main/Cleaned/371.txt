incomplete dot product dynamic computation scaling neural network inference machine learning approximate computing iot deep learning convolutional neural network propose use incomplete dot product idp dynamically adjust number input channel used each layer convolutional neural network during feedforward inference idp add monotonically non increasing coefficient referred profile channel during training profile order contribution each channel non increasing order inference time number channel used dynamically adjusted trade off accuracy lowered power consumption reduced latency by selecting only beginning subset channel this approach allows single network dynamically scale over computation range opposed training deploying multiple network support different level computation scaling additionally extend notion multiple profile each optimized some specific range computation scaling present experiment computation accuracy trade offs idp popular image classification model datasets demonstrate that mnist cifar 10 idp reduces computation significantly e g by 75 without significantly compromising accuracy argue that idp provides convenient effective mean device lower computation cost dynamically reflect current computation budget system example vgg 16 with 50 idp using only first 50 channel achieves 70 accuracy cifar 10 dataset compared standard network which achieves only 35 accuracy when using reduced channel set 