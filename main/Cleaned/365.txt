anytime exploitation straggler synchronous stochastic gradient descent distributed sgd stochastic gradient descent parallelized sgd straggler this paper propose an approach parallelizing synchronous stochastic gradient descent sgd that term anytime gradient anytime gradient designed exploit work completed by slow compute node or straggler many approach work completed by these node while only partial discarded completely maintain synchronization our approach each computational epoch fixed duration end each epoch worker send updated parameter vector master mode combination master weight each update by amount work done anytime gradient scheme robust both persistent non persistent straggler requires prior knowledge about processor ability show that scheme effectively exploit straggler outperforms existing method 