effect dataset size training tweet sentiment classifier sentiment analysis tweet mining classification big data using automated method labeling tweet sentiment large volume tweet labeled used train classifier million tweet could used train classifier however doing so computationally expensive thus it valuable establish how many tweet should utilized train classifier since using additional instance with gain performance waste resource this study seek find out how many tweet needed before significant improvement observed sentiment analysis when adding additional instance train evaluate classifier using c4 5 decision tree naïve bayes 5 nearest neighbor radial basis function network with seven datasets varying from 1000 243 000 instance model trained using four run 5 fold cross validation additionally conduct statistical test verify our observation examine impact limiting feature using frequency learner were found improve with dataset size with naïve bayes being best performing learner found that naïve bayes did not significantly benefit from using more than 81 000 instance best our knowledge this first study investigate how learner scale respect dataset size with result verified using statistical test multiple model trained each learner dataset size additionally investigated using feature frequency greatly reduce data grid size with either small increase or decrease classifier performance depending choice learner 