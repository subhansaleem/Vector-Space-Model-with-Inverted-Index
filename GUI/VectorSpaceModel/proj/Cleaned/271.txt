bayesian unif gradient bandit base learn acceler global optimis global optim multi arm bandit line learn multi strategi learn bandit base optimis scheme remark advantag over gradient base approach due their global perspect which elimin danger get stuck local optima howev continu optimis problem or problem with larg number action bandit base approach hinder by slow learn gradient base approach other hand navig quickli high dimension continu space through local optimis follow gradient fine grain step howev apart from be suscept local optima these scheme also less suit onlin learn due their relianc extens trial error befor optimum identifi contrast bandit algorithm seek identifi optim action global optima few step possibl thi paper propos bayesian approach that unifi abov two distinct paradigm one singl framework with aim combin their advantag heart our approach find stochast linear approxim function optimis where both gradient valu function explicitli captur thi model allow us learn from both noisi function gradient observ well predict these properti across action space support optimis further propos an accompani bandit driven explor scheme that use bayesian credibl bound trade off explor against exploit our empir result demonstr that by unifi bandit gradient base learn one obtain consist improv perform across wide spectrum problem environ furthermor even when gradient feedback unavail flexibl our model includ gradient predict still allow us outperform compet approach although with smaller margin 